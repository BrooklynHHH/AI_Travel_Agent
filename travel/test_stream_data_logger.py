#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµå¼æ•°æ®æµ‹è¯•å’Œæ—¥å¿—è®°å½•å™¨
ç”¨äºæµ‹è¯• run_enhanced_streaming_api å‡½æ•°çš„åŸå§‹æ•°æ®è¾“å‡º
"""

import sys
import os
import json
import time
import traceback
from datetime import datetime
from typing import Dict, Any, List, Generator
import logging

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ç›¸å…³æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# å¯¼å…¥æµ‹è¯•ç›®æ ‡æ¨¡å—
try:
    from enhanced_stream_api_V1 import run_enhanced_streaming_api
    from supervisor_agent import create_multi_turn_chat_supervisor
    from simple_multi_turn_system.config import SUPERVISOR_CONFIG
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–æ¨¡å—éƒ½å­˜åœ¨")
    sys.exit(1)

class StreamDataLogger:
    """æµå¼æ•°æ®è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "stream_test_logs"):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        
        Args:
            log_dir: æ—¥å¿—æ–‡ä»¶ç›®å½•
        """
        self.log_dir = log_dir
        self.test_start_time = datetime.now()
        self.chunk_count = 0
        self.total_data_size = 0
        self.data_types_seen = set()
        self.field_structures = {}
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        self.setup_loggers()
        
    def setup_loggers(self):
        """è®¾ç½®å„ç§æ—¥å¿—è®°å½•å™¨"""
        timestamp = self.test_start_time.strftime("%Y%m%d_%H%M%S")
        
        # åŸå§‹æ•°æ®æ—¥å¿—
        self.raw_data_file = open(
            os.path.join(self.log_dir, f"raw_stream_data_{timestamp}.log"), 
            'w', encoding='utf-8'
        )
        
        # ç»“æ„åˆ†ææ—¥å¿—
        self.structure_file = open(
            os.path.join(self.log_dir, f"data_structure_{timestamp}.log"), 
            'w', encoding='utf-8'
        )
        
        # æ€§èƒ½æŒ‡æ ‡æ—¥å¿—
        self.performance_file = open(
            os.path.join(self.log_dir, f"performance_metrics_{timestamp}.log"), 
            'w', encoding='utf-8'
        )
        
        # é”™è¯¯æ—¥å¿—
        logging.basicConfig(
            filename=os.path.join(self.log_dir, f"error_log_{timestamp}.log"),
            level=logging.ERROR,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # å†™å…¥æ—¥å¿—å¤´éƒ¨ä¿¡æ¯
        self._write_log_headers()
    
    def _write_log_headers(self):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯"""
        header_info = f"""
=== æµå¼æ•°æ®æµ‹è¯•æ—¥å¿— ===
æµ‹è¯•å¼€å§‹æ—¶é—´: {self.test_start_time.isoformat()}
æµ‹è¯•ç›®æ ‡: run_enhanced_streaming_api å‡½æ•°
é…ç½®å‚æ•°: {SUPERVISOR_CONFIG}
========================================

"""
        
        self.raw_data_file.write(header_info)
        self.structure_file.write(header_info)
        self.performance_file.write(header_info)
        
        # åˆ·æ–°ç¼“å†²åŒº
        self.raw_data_file.flush()
        self.structure_file.flush()
        self.performance_file.flush()
    
    def log_chunk(self, chunk: Dict[str, Any], chunk_timestamp: datetime):
        """
        è®°å½•å•ä¸ªæ•°æ®å—
        
        Args:
            chunk: æ•°æ®å—
            chunk_timestamp: æ•°æ®å—æ—¶é—´æˆ³
        """
        self.chunk_count += 1
        
        try:
            # åºåˆ—åŒ–æ•°æ®å—
            chunk_json = json.dumps(chunk, ensure_ascii=False, indent=2)
            chunk_size = len(chunk_json.encode('utf-8'))
            self.total_data_size += chunk_size
            
            # è®°å½•åŸå§‹æ•°æ®
            self._log_raw_data(chunk, chunk_json, chunk_timestamp, chunk_size)
            
            # åˆ†ææ•°æ®ç»“æ„
            self._analyze_structure(chunk, chunk_timestamp)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self._log_performance(chunk_timestamp, chunk_size)
            
            # å®æ—¶è¾“å‡ºè¿›åº¦
            print(f"[{chunk_timestamp.strftime('%H:%M:%S.%f')[:-3]}] "
                  f"Chunk #{self.chunk_count} | Size: {chunk_size} bytes | "
                  f"Type: {type(chunk).__name__}")
            
        except Exception as e:
            logging.error(f"è®°å½•æ•°æ®å—æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}")
            print(f"âŒ è®°å½•æ•°æ®å— #{self.chunk_count} æ—¶å‡ºé”™: {str(e)}")
    
    def _log_raw_data(self, chunk: Dict[str, Any], chunk_json: str, 
                      timestamp: datetime, size: int):
        """è®°å½•åŸå§‹æ•°æ®"""
        log_entry = f"""
--- CHUNK #{self.chunk_count} ---
æ—¶é—´æˆ³: {timestamp.isoformat()}
æ•°æ®å¤§å°: {size} bytes
åŸå§‹æ•°æ®:
{chunk_json}
---

"""
        self.raw_data_file.write(log_entry)
        self.raw_data_file.flush()
    
    def _analyze_structure(self, chunk: Dict[str, Any], timestamp: datetime):
        """åˆ†ææ•°æ®ç»“æ„"""
        try:
            structure_info = self._extract_structure_info(chunk)
            
            log_entry = f"""
--- ç»“æ„åˆ†æ #{self.chunk_count} ---
æ—¶é—´æˆ³: {timestamp.isoformat()}
æ•°æ®ç±»å‹: {type(chunk).__name__}
å­—æ®µç»“æ„: {json.dumps(structure_info, ensure_ascii=False, indent=2)}
---

"""
            self.structure_file.write(log_entry)
            self.structure_file.flush()
            
            # æ›´æ–°å…¨å±€ç»“æ„ç»Ÿè®¡
            self._update_structure_stats(structure_info)
            
        except Exception as e:
            logging.error(f"åˆ†ææ•°æ®ç»“æ„æ—¶å‡ºé”™: {str(e)}")
    
    def _extract_structure_info(self, obj: Any, max_depth: int = 5, current_depth: int = 0) -> Dict[str, Any]:
        """é€’å½’æå–æ•°æ®ç»“æ„ä¿¡æ¯"""
        if current_depth >= max_depth:
            return {"type": type(obj).__name__, "truncated": True}
        
        if isinstance(obj, dict):
            return {
                "type": "dict",
                "keys": list(obj.keys()),
                "fields": {
                    key: self._extract_structure_info(value, max_depth, current_depth + 1)
                    for key, value in obj.items()
                }
            }
        elif isinstance(obj, list):
            return {
                "type": "list",
                "length": len(obj),
                "item_types": [type(item).__name__ for item in obj[:5]],  # åªåˆ†æå‰5ä¸ªå…ƒç´ 
                "sample_structure": self._extract_structure_info(obj[0], max_depth, current_depth + 1) if obj else None
            }
        else:
            return {
                "type": type(obj).__name__,
                "value_sample": str(obj)[:100] if len(str(obj)) > 100 else str(obj)
            }
    
    def _update_structure_stats(self, structure_info: Dict[str, Any]):
        """æ›´æ–°ç»“æ„ç»Ÿè®¡ä¿¡æ¯"""
        data_type = structure_info.get("type", "unknown")
        self.data_types_seen.add(data_type)
        
        if data_type == "dict":
            keys = structure_info.get("keys", [])
            for key in keys:
                if key not in self.field_structures:
                    self.field_structures[key] = {"count": 0, "types": set()}
                self.field_structures[key]["count"] += 1
                field_type = structure_info.get("fields", {}).get(key, {}).get("type", "unknown")
                self.field_structures[key]["types"].add(field_type)
    
    def _log_performance(self, timestamp: datetime, chunk_size: int):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        elapsed_time = (timestamp - self.test_start_time).total_seconds()
        avg_chunk_size = self.total_data_size / self.chunk_count if self.chunk_count > 0 else 0
        
        log_entry = f"""
æ—¶é—´æˆ³: {timestamp.isoformat()}
Chunk #{self.chunk_count}
å½“å‰å—å¤§å°: {chunk_size} bytes
ç´¯è®¡æ•°æ®é‡: {self.total_data_size} bytes
å¹³å‡å—å¤§å°: {avg_chunk_size:.2f} bytes
ç´¯è®¡è€—æ—¶: {elapsed_time:.3f} seconds
å¤„ç†é€Ÿç‡: {self.chunk_count / elapsed_time:.2f} chunks/sec
æ•°æ®é€Ÿç‡: {self.total_data_size / elapsed_time:.2f} bytes/sec
---

"""
        self.performance_file.write(log_entry)
        self.performance_file.flush()
    
    def generate_summary(self):
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        test_end_time = datetime.now()
        total_duration = (test_end_time - self.test_start_time).total_seconds()
        
        summary = {
            "test_info": {
                "start_time": self.test_start_time.isoformat(),
                "end_time": test_end_time.isoformat(),
                "total_duration_seconds": total_duration
            },
            "data_statistics": {
                "total_chunks": self.chunk_count,
                "total_data_size_bytes": self.total_data_size,
                "average_chunk_size_bytes": self.total_data_size / self.chunk_count if self.chunk_count > 0 else 0,
                "processing_rate_chunks_per_sec": self.chunk_count / total_duration if total_duration > 0 else 0,
                "data_rate_bytes_per_sec": self.total_data_size / total_duration if total_duration > 0 else 0
            },
            "structure_analysis": {
                "data_types_seen": list(self.data_types_seen),
                "field_statistics": {
                    key: {
                        "occurrence_count": stats["count"],
                        "data_types": list(stats["types"])
                    }
                    for key, stats in self.field_structures.items()
                }
            },
            "config_used": SUPERVISOR_CONFIG
        }
        
        # ä¿å­˜æ€»ç»“åˆ°JSONæ–‡ä»¶
        timestamp = self.test_start_time.strftime("%Y%m%d_%H%M%S")
        summary_file = os.path.join(self.log_dir, f"test_summary_{timestamp}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        return summary
    
    def close(self):
        """å…³é—­æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        try:
            self.raw_data_file.close()
            self.structure_file.close()
            self.performance_file.close()
        except Exception as e:
            logging.error(f"å…³é—­æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")


def test_stream_api_with_logging(user_input: str, test_name: str = "default"):
    """
    æµ‹è¯•æµå¼APIå¹¶è®°å½•æ‰€æœ‰æ•°æ®
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        test_name: æµ‹è¯•åç§°
    """
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•: {test_name}")
    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
    print(f"âš™ï¸ é…ç½®å‚æ•°: {SUPERVISOR_CONFIG}")
    print("=" * 60)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    log_dir = f"stream_test_logs/{test_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger = StreamDataLogger(log_dir)
    
    try:
        # åˆå§‹åŒ–supervisor
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–supervisor...")
        supervisor = create_multi_turn_chat_supervisor()
        print("âœ… Supervisoråˆå§‹åŒ–æˆåŠŸ")
        
        # å¼€å§‹æµå¼å¤„ç†
        print("ğŸŒŠ å¼€å§‹æµå¼å¤„ç†...")
        start_time = time.time()
        
        for chunk in run_enhanced_streaming_api(
            user_input=user_input,
            supervisor=supervisor,
            **SUPERVISOR_CONFIG
        ):
            chunk_timestamp = datetime.now()
            logger.log_chunk(chunk, chunk_timestamp)
        
        end_time = time.time()
        print(f"\nâœ… æµå¼å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.3f} ç§’")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        logging.error(f"æµ‹è¯•å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        
    finally:
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ€»ç»“...")
        summary = logger.generate_summary()
        logger.close()
        
        # æ‰“å°æ€»ç»“ä¿¡æ¯
        print("\n" + "=" * 60)
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“¦ æ€»æ•°æ®å—æ•°: {summary['data_statistics']['total_chunks']}")
        print(f"ğŸ“ æ€»æ•°æ®é‡: {summary['data_statistics']['total_data_size_bytes']} bytes")
        print(f"âš¡ å¤„ç†é€Ÿç‡: {summary['data_statistics']['processing_rate_chunks_per_sec']:.2f} chunks/sec")
        print(f"ğŸ” å‘ç°çš„æ•°æ®ç±»å‹: {', '.join(summary['structure_analysis']['data_types_seen'])}")
        print(f"ğŸ“ æ—¥å¿—ä¿å­˜ä½ç½®: {log_dir}")
        
        return summary


def run_multiple_tests():
    """è¿è¡Œå¤šä¸ªæµ‹è¯•ç”¨ä¾‹"""
    test_cases = [
        {
            "name": "simple_query",
            "input": "æ¨èåŒ—äº¬çš„æ™¯ç‚¹",
            "description": "ç®€å•æ™¯ç‚¹æ¨èæŸ¥è¯¢"
        },
        {
            "name": "complex_planning",
            "input": "æˆ‘æƒ³è¦ä¸€ä¸ª3å¤©çš„åŒ—äº¬æ—…æ¸¸è®¡åˆ’ï¼ŒåŒ…æ‹¬æ•…å®«ã€å¤©å›ã€é¢å’Œå›­",
            "description": "å¤æ‚æ—…æ¸¸è§„åˆ’æŸ¥è¯¢"
        },
        {
            "name": "detailed_request",
            "input": "æˆ‘å’Œå¥³æœ‹å‹è®¡åˆ’ä¸‹ä¸ªæœˆå»æ­å·ç©3å¤©ï¼Œé¢„ç®—3000å…ƒï¼Œå–œæ¬¢å†å²æ–‡åŒ–å’Œç¾é£Ÿï¼Œè¯·å¸®æˆ‘åˆ¶å®šè¯¦ç»†çš„æ—…æ¸¸è®¡åˆ’",
            "description": "è¯¦ç»†éœ€æ±‚çš„æ—…æ¸¸è§„åˆ’"
        }
    ]
    
    print("ğŸ¯ å¼€å§‹æ‰¹é‡æµ‹è¯•æµå¼API")
    print(f"ğŸ“‹ å…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    all_summaries = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_cases)}: {test_case['description']}")
        print(f"{'='*80}")
        
        try:
            summary = test_stream_api_with_logging(
                user_input=test_case["input"],
                test_name=test_case["name"]
            )
            all_summaries.append({
                "test_case": test_case,
                "summary": summary
            })
            
            print(f"âœ… æµ‹è¯•ç”¨ä¾‹ {i} å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•ç”¨ä¾‹ {i} å¤±è´¥: {str(e)}")
            all_summaries.append({
                "test_case": test_case,
                "error": str(e)
            })
        
        # æµ‹è¯•é—´éš”
        if i < len(test_cases):
            print("\nâ³ ç­‰å¾…5ç§’åå¼€å§‹ä¸‹ä¸€ä¸ªæµ‹è¯•...")
            time.sleep(5)
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    generate_overall_report(all_summaries)
    
    return all_summaries


def generate_overall_report(all_summaries: List[Dict[str, Any]]):
    """ç”Ÿæˆæ€»ä½“æµ‹è¯•æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"stream_test_logs/overall_report_{timestamp}.json"
    
    # åˆ›å»ºç›®å½•
    os.makedirs("stream_test_logs", exist_ok=True)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    successful_tests = [s for s in all_summaries if "summary" in s]
    failed_tests = [s for s in all_summaries if "error" in s]
    
    total_chunks = sum(s["summary"]["data_statistics"]["total_chunks"] for s in successful_tests)
    total_data_size = sum(s["summary"]["data_statistics"]["total_data_size_bytes"] for s in successful_tests)
    
    report = {
        "test_overview": {
            "timestamp": timestamp,
            "total_tests": len(all_summaries),
            "successful_tests": len(successful_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(successful_tests) / len(all_summaries) * 100 if all_summaries else 0
        },
        "aggregate_statistics": {
            "total_chunks_across_all_tests": total_chunks,
            "total_data_size_bytes_across_all_tests": total_data_size,
            "average_chunks_per_test": total_chunks / len(successful_tests) if successful_tests else 0,
            "average_data_size_per_test": total_data_size / len(successful_tests) if successful_tests else 0
        },
        "detailed_results": all_summaries
    }
    
    # ä¿å­˜æŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ€»ä½“æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ“Š æ€»ä½“æµ‹è¯•æŠ¥å‘Š")
    print(f"{'='*80}")
    print(f"ğŸ§ª æ€»æµ‹è¯•æ•°: {report['test_overview']['total_tests']}")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {report['test_overview']['successful_tests']}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {report['test_overview']['failed_tests']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {report['test_overview']['success_rate']:.1f}%")
    print(f"ğŸ“¦ æ€»æ•°æ®å—: {report['aggregate_statistics']['total_chunks_across_all_tests']}")
    print(f"ğŸ“ æ€»æ•°æ®é‡: {report['aggregate_statistics']['total_data_size_bytes_across_all_tests']} bytes")
    print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")


def quick_test():
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªç”¨ä¾‹"""
    return test_stream_api_with_logging(
        user_input="æ¨èæ­å·çš„æ™¯ç‚¹",
        test_name="quick_test"
    )


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æµå¼APIæ•°æ®æµ‹è¯•å·¥å…·")
    parser.add_argument("--mode", choices=["quick", "single", "batch"], default="quick",
                       help="æµ‹è¯•æ¨¡å¼: quick(å¿«é€Ÿæµ‹è¯•), single(å•ä¸ªæµ‹è¯•), batch(æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--input", type=str, help="ç”¨æˆ·è¾“å…¥(singleæ¨¡å¼ä½¿ç”¨)")
    parser.add_argument("--name", type=str, default="custom_test", help="æµ‹è¯•åç§°(singleæ¨¡å¼ä½¿ç”¨)")
    
    args = parser.parse_args()
    
    if args.mode == "quick":
        print("ğŸš€ è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        quick_test()
    elif args.mode == "single":
        if not args.input:
            print("âŒ singleæ¨¡å¼éœ€è¦æä¾› --input å‚æ•°")
            sys.exit(1)
        print(f"ğŸš€ è¿è¡Œå•ä¸ªæµ‹è¯•: {args.name}")
        test_stream_api_with_logging(args.input, args.name)
    elif args.mode == "batch":
        print("ğŸš€ è¿è¡Œæ‰¹é‡æµ‹è¯•...")
        run_multiple_tests()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
